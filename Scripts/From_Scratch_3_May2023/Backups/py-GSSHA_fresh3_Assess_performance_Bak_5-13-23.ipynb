{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce4317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "from distutils.dir_util import copy_tree\n",
    "import distutils\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e45d3e",
   "metadata": {},
   "source": [
    "### Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_Describe_stats(data, valType):\n",
    "    mindate = data['datetime'].min()\n",
    "    maxdate = data['datetime'].max()\n",
    "    daterez = data['datetime'][1]-data['datetime'][0]\n",
    "    minval = data[valType].min()\n",
    "    maxval = data[valType].max()\n",
    "    aveval = data[valType].mean()\n",
    "    medval = data[valType].median()\n",
    "    print(\"Date range = {} to {}\".format(mindate, maxdate))\n",
    "    print(\"Timestep is {}\".format(daterez))\n",
    "    print(\"Value range is {} to {}\".format(minval, maxval))   \n",
    "    print(\"Average value is {}\".format(aveval))\n",
    "    print(\"Median value is {}\".format(medval)) \n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "### The rain gage file maker function for a single rain gauge ### \n",
    "# Input_Precip_df - A dataframe of timerseries precipitation data \n",
    "# Precip_column_Name - The column in the above Input_Precip_df to use\n",
    "# gagFileFolder-  The relative path to the gag file formatted as \n",
    "# StartDate - Desired start date of the run (note will work if no hours or minutes but best to add them)\n",
    "# EndDate  - Desired end date of the run \n",
    "# Lat lon of the gauge site, important for GSSHA calculations\n",
    "# GageName - name of gauge, not so important \n",
    "# RainSeries_timestep_Mins - The timestep of the input rainfall data from Input_Precip_df\n",
    "# ImpPrecip_units - Units of the precip df, this converts it to mm before running in GSSHA as mm is the default GSSHA units\n",
    "    \n",
    "def make_rain_gag_file(PrjName, Input_Precip_df, Precip_column_Name, gagFileFolder, StartDate, EndDate, Lat, Lon, \n",
    "                       GageName, RainSeries_timestep_Mins, ImpPrecip_units=\"Inches\"):\n",
    "\n",
    "    Full_Frame = Input_Precip_df   # read in data \n",
    "    SliceFrame = Full_Frame[StartDate:EndDate]\n",
    "\n",
    "    # Format date string for dumb file \n",
    "    SliceFrame_format = SliceFrame.copy()\n",
    "    SliceFrame_format['da'] = SliceFrame_format.index.strftime('%Y %m %d %H %M')\n",
    "    SliceFrame_format['datedumb'] = SliceFrame_format['da'].str[:]\n",
    "\n",
    "    # Pull extranious columns \n",
    "    SliceFrame_format = SliceFrame_format[['datedumb', Precip_column_Name]]\n",
    "\n",
    "    # Turn rain in to rain MM \n",
    "    if ImpPrecip_units == \"Inches\": \n",
    "        SliceFrame_format[Precip_column_Name+\"_mm\"] = SliceFrame_format[Precip_column_Name]*25.4 \n",
    "        Precip_column_Name = Precip_column_Name+\"_mm\"\n",
    "        \n",
    "    # Round off the number of significant figs\n",
    "    sigfigs = 3\n",
    "    SliceFrame_format[Precip_column_Name] = SliceFrame_format[Precip_column_Name].round(sigfigs).apply(lambda x: \n",
    "                                                                                                 '{0:g}'.format(float(x)))\n",
    "    # Put the GAGES card on EVery single row \n",
    "    SliceFrame_format[\"trash\"] = \"GAGES\"\n",
    "\n",
    "    # reorder columns \n",
    "    SliceFrame_format = SliceFrame_format[[\"trash\", \"datedumb\", Precip_column_Name]]\n",
    "\n",
    "    # Print it off to a txt file with no header, no index and space separator\n",
    "    FileNamePlace = os.path.join(gagFileFolder, \"{}.gag\".format(PrjName))\n",
    "    SliceFrame_format.to_csv(FileNamePlace, index=False, sep=' ', header = False) \n",
    "\n",
    "    # Remove dumb double quotes \n",
    "    with open(FileNamePlace,'r') as file:\n",
    "        data = file.read()\n",
    "        data = data.replace('\"','')\n",
    "    with open(FileNamePlace,'w') as file:    \n",
    "        file.write(data)\n",
    "\n",
    "    # Create the required header lines for WMS gag  files \n",
    "    AddLine = 'EVENT \"Rain Gage\" \\nNRGAG 1 \\nNRPDS {}\\nCOORD {} {} \"{}\"'.format(len(SliceFrame_format),Lat, Lon, GageName)  \n",
    "\n",
    "    with open(FileNamePlace, \"r+\") as f:\n",
    "        old = f.read() # read everything in the file\n",
    "        f.seek(0) # rewind\n",
    "        f.write(\"{}\\n\".format(AddLine) + old) # write the new line before\n",
    "\n",
    "    # Calculate the number of minutes to run the damn thing. \n",
    "    Run_Length_mins = RainSeries_timestep_Mins*len(SliceFrame_format)\n",
    "    print(\"Simulation time is {} minutes\".format(Run_Length_mins))\n",
    "    print(\"number of time rows is {}\".format(len(SliceFrame_format)))\n",
    "    \n",
    "    return Run_Length_mins, SliceFrame_format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Process outlet file from GSSHA into a pandas dataframe ###\n",
    "# StartDate - has to have hours and minutes e.g. 00:00\n",
    "# OutletFile  - the gssha .otl file from the project \n",
    "\n",
    "def process_otl_file(StartDate, OutletFile):\n",
    "\n",
    "    # Create the start time object to enumerate the number of minutes in the outlet file \n",
    "    StartDateTime = datetime.strptime(StartDate, '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # read in the outlet file \n",
    "    OutHydro = pd.read_csv(OutletFile, names=[\"Minutes\", \"CFS\"], delim_whitespace=True)\n",
    "    \n",
    "    # da magic: Turn stupid minutes into useful datetime objects \n",
    "    OutHydro[\"datetime\"] = OutHydro[\"Minutes\"].apply(lambda x: StartDateTime + timedelta(minutes=x))\n",
    "    \n",
    "    # Set the index to the date\n",
    "    OutHydro.set_index(\"datetime\", inplace=True)\n",
    "    \n",
    "    return OutHydro\n",
    "\n",
    "\n",
    "\n",
    "#### Process streamflow datasets from da streamflow dataframe  ### \n",
    "\n",
    "def Isolate_Stream_Data(Input_Stream_df, StreamFlow_column_Name, StartDate, EndDate, \n",
    "                        StreamFlowObs_resample_Timestep_mins = 60):\n",
    "    SliceFrame = Input_Stream_df[StartDate:EndDate]\n",
    "    \n",
    "    # USGS data is amazingly terrible, it does not have consistent timesteps, fix this issue here\n",
    "    mins = StreamFlowObs_resample_Timestep_mins\n",
    "    SliceFrame = SliceFrame.resample('{}T'.format(mins)).mean().interpolate()\n",
    "\n",
    "    return SliceFrame[[StreamFlow_column_Name]]\n",
    "\n",
    "\n",
    "\n",
    "### Calculate NSE of predictions and obs  #### \n",
    "def nse(predictions, targets):\n",
    "    return (1-(np.sum((predictions-targets)**2)/np.sum((targets-np.mean(targets))**2)))\n",
    "\n",
    "\n",
    "#### Open up the parameters file and replace certain keys with certain values   #### \n",
    "def cmt_prama_jama(MapTableFile, Param, Val):\n",
    "    with open(MapTableFile, 'r') as file :    # Read in the file \n",
    "        filedata = file.read()\n",
    "\n",
    "    filedata = filedata.replace(Param, Val)  # Replace the target paramater(s)\n",
    "\n",
    "    with open(MapTableFile, 'w') as file:   # Write the file out again\n",
    "        file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e411843",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### RUN THIS CELL ONLY WHEN SAVING A FRESH PROJECT FILE FROM WMS in the RUN directory!\n",
    "\n",
    "#### Nuke out the PRISTINE_MODEL_COPY directory to start fresh\n",
    "#for f in os.listdir(\"PRISTINE_MODEL_COPY\"):  os.remove(os.path.join(\"PRISTINE_MODEL_COPY\", f))\n",
    "\n",
    "#### This will copy out the project files to the working directory  \n",
    "#copy_tree(\"RUN\", 'PRISTINE_MODEL_COPY')\n",
    "\n",
    "#### Copy in GSSHA exe\n",
    "#distutils.file_util.copy_file(os.path.join(\".\", \"data\", \"gssha.exe\"), 'PRISTINE_MODEL_COPY')\n",
    "\n",
    "#### Nuke out the RUN directory to start fresh\n",
    "#for f in os.listdir(\"RUN\"):  os.remove(os.path.join(\"RUN\", f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e76c00",
   "metadata": {},
   "source": [
    "# Rainfall data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b053bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the three closest rainfall stations into a rainfall file \n",
    "\n",
    "# Import KAHAKULOA station \n",
    "KAHAKULOARain = pd.read_csv(os.path.join(\".\", \"data/precip\", 'KHKH1.csv'))\n",
    "KAHAKULOARain['DateTime'] = pd.to_datetime(KAHAKULOARain['DateTime'], errors='coerce')\n",
    "# Clean data \n",
    "KAHAKULOARain = KAHAKULOARain[['DateTime', 'RF_mm']]\n",
    "KAHAKULOARain.rename(columns={'DateTime': 'datetime'}, inplace=True)\n",
    "# Plot data \n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(KAHAKULOARain['datetime'],KAHAKULOARain['RF_mm'], '-',label='Rainfall Inches')\n",
    "TS_Describe_stats(KAHAKULOARain, 'RF_mm')\n",
    "\n",
    "# Import PUU_KUKIv2 station \n",
    "PUU_KUKIv2 = pd.read_csv(os.path.join(\".\", \"data/precip\", 'USGS_uv205327156351102.csv'))\n",
    "PUU_KUKIv2['DateTime'] = pd.to_datetime(PUU_KUKIv2['DateTime'], errors='coerce')\n",
    "# Clean data \n",
    "PUU_KUKIv2 = PUU_KUKIv2[['DateTime', 'RF_mm']]\n",
    "PUU_KUKIv2.rename(columns={'DateTime': 'datetime'}, inplace=True)\n",
    "# Plot data \n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(PUU_KUKIv2['datetime'],PUU_KUKIv2['RF_mm'], '-',label='Rainfall Inches')\n",
    "TS_Describe_stats(PUU_KUKIv2, 'RF_mm')\n",
    "\n",
    "# Import WAILUKU_Rain station \n",
    "WAILUKU_Rain = pd.read_csv(os.path.join(\"..\", \"..\", \"Data/Rainfall_data/Yufen_paper\", 'WUKH1.csv'))\n",
    "WAILUKU_Rain['DateTime'] = pd.to_datetime(WAILUKU_Rain['DateTime'], errors='coerce')\n",
    "# Clean data \n",
    "WAILUKU_Rain = WAILUKU_Rain[['DateTime', 'RF_mm']]\n",
    "WAILUKU_Rain.rename(columns={'DateTime': 'datetime'}, inplace=True)\n",
    "# Plot data \n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(WAILUKU_Rain['datetime'],WAILUKU_Rain['RF_mm'], '-',label='Rainfall Inches')\n",
    "TS_Describe_stats(WAILUKU_Rain, 'RF_mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT Resample rainfall, (it makes the nans into 0s)combine \n",
    "KAHAKULOARain_set = KAHAKULOARain.set_index('datetime')\n",
    "KAHAKULOARain_set.rename(columns={'RF_mm': 'KAHAKULOARain'}, inplace=True)\n",
    "\n",
    "PUU_KUKIv2_set = PUU_KUKIv2.set_index('datetime')\n",
    "PUU_KUKIv2_set.rename(columns={'RF_mm': 'PUU_KUKIv2'}, inplace=True)\n",
    "\n",
    "WAILUKU_Rain_set = WAILUKU_Rain.set_index('datetime')\n",
    "WAILUKU_Rain_set.rename(columns={'RF_mm': 'WAILUKU_Rain'}, inplace=True)\n",
    "\n",
    "# Merge themm all togehter\n",
    "m1 = PUU_KUKIv2_set.join(KAHAKULOARain_set, how='outer')\n",
    "Merged_Rainfall_mm = m1.join(WAILUKU_Rain_set)\n",
    "\n",
    "# Create an average column of the three\n",
    "Merged_Rainfall_mm['MEAN_all']= Merged_Rainfall_mm.mean(axis=1)\n",
    "\n",
    "\n",
    "# Plot em all \n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(Merged_Rainfall_mm['PUU_KUKIv2'], '-', c='orange', alpha=0.5, label=\"PUU_KUKIv2\")\n",
    "ax.plot(Merged_Rainfall_mm['KAHAKULOARain'], '-', c='b', alpha=0.5, label=\"KAHAKULOARain\")\n",
    "ax.plot(Merged_Rainfall_mm['WAILUKU_Rain'], '-', c='pink', alpha=0.5, label=\"WAILUKU_Rain\")\n",
    "ax.plot(Merged_Rainfall_mm['MEAN_all'], ':', c='k', alpha=0.2, label=\"MEAN_all\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e4583",
   "metadata": {},
   "source": [
    "# Process Stream files \n",
    "\n",
    "Note that this leaves them at different time steps resolutions\n",
    "Will need to resample to the GSSHA output timesep when using them for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16480f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Wailuku data \n",
    "WailukuFlow = pd.read_csv(os.path.join(\".\", \"data/Streamflow\", 'FlowWailukuKepaniwai_2000-01-01_to_2023-05-01.csv'))\n",
    "WailukuFlow['value_time'] = pd.to_datetime(WailukuFlow['value_time'], errors='coerce')\n",
    "# Clean data \n",
    "WailukuFlow = WailukuFlow[['value_time', 'value']]\n",
    "WailukuFlow.rename(columns={'value': 'WailukuFlow', 'value_time':'datetime'}, inplace=True)\n",
    "# Plot data \n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(WailukuFlow['datetime'],WailukuFlow['WailukuFlow'], '-',label='WailukuFlow')\n",
    "TS_Describe_stats(WailukuFlow, \"WailukuFlow\")\n",
    "WailukuFlow = WailukuFlow.set_index('datetime')  # set index to datetime index for use later \n",
    "\n",
    "\n",
    "# Import Waihehe data \n",
    "WaiheheFlow = pd.read_csv(os.path.join(\".\", \"data/Streamflow\", 'FlowWaihehe_2000-01-01_to_2023-05-01.csv'))\n",
    "WaiheheFlow['value_time'] = pd.to_datetime(WaiheheFlow['value_time'], errors='coerce')\n",
    "# Clean data \n",
    "WaiheheFlow = WaiheheFlow[['value_time', 'value']]\n",
    "WaiheheFlow.rename(columns={'value': 'WaiheheFlow', 'value_time':'datetime'}, inplace=True)\n",
    "# Plot data \n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(WaiheheFlow['datetime'],WaiheheFlow['WaiheheFlow'], '-',label='WaiheheFlow')\n",
    "WaiheheFlow = WaiheheFlow.set_index('datetime')  # set index to datetime index for use later \n",
    "\n",
    "\n",
    "# Import SWaiehu data   ( note that this file was downloaded manually the API didnt work so the import is a little different than above)\n",
    "SWaiehuFlow = pd.read_csv(os.path.join(\".\", \"data/Streamflow\", 'Flow_SWaiehu_2022_to_2023.csv'))\n",
    "SWaiehuFlow['value_time'] = pd.to_datetime(SWaiehuFlow['value_time'], errors='coerce')\n",
    "# Thre were some strings in the value column, freakin USGS....\n",
    "SWaiehuFlow['value'] = SWaiehuFlow['value'].apply(lambda x: pd.to_numeric(x, errors='coerce') )\n",
    "# Clean data \n",
    "SWaiehuFlow = SWaiehuFlow[['value_time', 'value']]\n",
    "SWaiehuFlow.rename(columns={'value': 'SWaiehuFlow', 'value_time':'datetime'}, inplace=True)\n",
    "# Plot data \n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(SWaiehuFlow['datetime'],SWaiehuFlow['SWaiehuFlow'], '-',label='SWaiehuFlow')\n",
    "TS_Describe_stats(SWaiehuFlow, 'SWaiehuFlow')\n",
    "SWaiehuFlow = SWaiehuFlow.set_index('datetime')  # set index to datetime index for use later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd6ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b0042cb",
   "metadata": {},
   "source": [
    "# Mossta loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a38627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set up to iterate over and run the thing \n",
    "\n",
    "# Static parameters for rain function \n",
    "Input_Precip_df = Merged_Rainfall_mm\n",
    "Precip_column_Name = \"PUU_KUKIv2\"\n",
    "ImpPrecip_units = \"mm\"\n",
    "gagFileFolder = os.path.join(\".\", \"RUN\")\n",
    "PrjName = \"Iao_v1_PrecipGag_50m\"\n",
    "Lat = \"753250.0\" \n",
    "Lon = \"2310140.0\"\n",
    "GageName = \"gag_face\"\n",
    "RainSeries_timestep_Mins = 60\n",
    "\n",
    "# Static parameters for stream obs function\n",
    "Input_Stream_df = WailukuFlow\n",
    "StreamFlow_column_Name = \"WailukuFlow\"\n",
    "StreamFlowObs_resample_Timestep_mins = 60   # note this will also be used to change HYD_FREQ in .prj file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493513",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save lists \n",
    "Param_Vals_list = []\n",
    "run_time_list =[]\n",
    "NSE_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modify roughness parameters \n",
    "rough_vals_list = [\"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", \"0.6\", \"0.7\"]\n",
    "\n",
    "# File where the roughness parameters are defined\n",
    "MapTableFile = os.path.join(\".\", \"RUN\", \"{}.cmt\".format(PrjName))\n",
    "\n",
    "\n",
    "\n",
    "# List of parameters in the model\n",
    "Param_list = [\"-107.000000\", \n",
    "              \"-108.000000\", \n",
    "              \"-1014.000000\", \n",
    "              \"-1015.000000\", \n",
    "              \"-1016.000000\", \n",
    "              \"-1017.000000\", \n",
    "              \"-1021.000000\", \n",
    "              \"-1022.000000\"]\n",
    "\n",
    "for i in rough_vals_list:\n",
    "\n",
    "    # Switch out each parameter with the Uniform value \n",
    "    for Param in Param_list:\n",
    "        cmt_prama_jama(MapTableFile, Param, i)\n",
    "        \n",
    "### Note even if not modifying these parameters need to run ALL code used to create model after the WMS save   \n",
    "### Since its all erased at the end of the block \n",
    "\n",
    "    # rain gauge maker code block ready to loop over start and end dates\n",
    "\n",
    "    StartDate = \"2018-02-17 00:00\"\n",
    "    EndDate = \"2018-02-21 00:00\"\n",
    "\n",
    "    Run_Length_mins, SlicedRainFrame = make_rain_gag_file(PrjName, Input_Precip_df, Precip_column_Name, gagFileFolder, StartDate, EndDate, Lat, Lon, \n",
    "                           GageName, RainSeries_timestep_Mins, ImpPrecip_units)\n",
    "\n",
    "    # Modify the .prj file to change the TOT_TIME card to whatever the total number of minutes output by the raingag function outputs\n",
    "    # read in the .prj file as a 1 column csv\n",
    "    df = pd.read_csv(os.path.join(\".\", \"RUN\", \"{}.prj\".format(PrjName)), names=[\"moo\"] )  # note names=moo is to make a column that will then get chopped off by numpy savetxt\n",
    "\n",
    "    # ID row of and change the TOT_TIME variable\n",
    "    singleCol = df.columns[0]                # this is the name of the single column\n",
    "    idx_tottime = df.loc[df[singleCol].str.contains(\"TOT_TIME\", case=False)].index[0]  # this identifies the index of the TOT_TIME card\n",
    "    # Set the total time value to something else. \n",
    "    df.loc[idx_tottime] = \"TOT_TIME      {}\".format(Run_Length_mins)\n",
    "\n",
    "    # ID row of and change the HYD_FREQ variable\n",
    "    singleCol = df.columns[0]                # this is the name of the single column\n",
    "    idx_HYD_FREQ = df.loc[df[singleCol].str.contains(\"HYD_FREQ\", case=False)].index[0]  # this identifies the index of the HYD_FREQ card\n",
    "    # Set the total time value to something else. \n",
    "    df.loc[idx_HYD_FREQ] = \"HYD_FREQ      {}\".format(StreamFlowObs_resample_Timestep_mins)\n",
    "\n",
    "    # Save the df back to a prj file (the np formulation seems to write better than the pd to csv one)\n",
    "    np.savetxt(os.path.join(\".\", \"RUN\", \"{}.prj\".format(PrjName)), df.values, fmt=\"%s\")\n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "\n",
    "    ##########    RUN THE MODEL   #################\n",
    "    # Run the GSSHA process in the command line \n",
    "    start_time = timeit.default_timer()     ###### Timer function start\n",
    "\n",
    "    os.chdir(os.path.join(\".\", \"RUN\"))         # Chanfge into run directoy \n",
    "    subprocess.call('gssha.exe {}.prj'.format(PrjName), shell=True)\n",
    "    os.chdir(os.path.join(\"..\"))                # Change back out of run directory \n",
    "\n",
    "    elapsed = timeit.default_timer() - start_time        ###### Timer function End, now time in seconds lives in elapsed\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Postprocessing block \n",
    "    # Process the Run's Output hydrograph into a dataframe \n",
    "    OutletFile = os.path.join(\".\", \"RUN\", PrjName)+\".otl\"\n",
    "    OutHydro = process_otl_file(StartDate, OutletFile)\n",
    "\n",
    "    # slice the desired streamflow obs to the correct period \n",
    "    SlicedStreamflow_df = Isolate_Stream_Data(Input_Stream_df, StreamFlow_column_Name, StartDate, EndDate, \n",
    "                                              StreamFlowObs_resample_Timestep_mins)\n",
    "\n",
    "    # NSE calculation          # Note this has hourly resampling hard coded in, not sure if need to change someday \n",
    "    predictions = OutHydro\n",
    "    targets = SlicedStreamflow_df.resample('60T').mean().interpolate()\n",
    "    NSE_Frame = predictions.join(targets, how='inner')\n",
    "    NSE_stat = nse(NSE_Frame['CFS'].values, NSE_Frame[StreamFlow_column_Name].values)\n",
    "    print(\"The NSE is {}\".format(NSE_stat))\n",
    "\n",
    "\n",
    "    # plot em \n",
    "    # For some reason need to cast the rainfall as a number not object?\n",
    "    SlicedRainFrame[Precip_column_Name] = SlicedRainFrame[Precip_column_Name].apply(lambda x: float(x)) \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax2 = ax.twinx()\n",
    "    lns0 = ax2.plot(SlicedRainFrame[Precip_column_Name], '.', c='b', alpha=0.4, label=\"Rainfall \")   # Plot rainfall \n",
    "    lns1 = ax.plot(SlicedStreamflow_df[StreamFlow_column_Name], '-', c='g', alpha=0.5, label=\"Observed Waihehe flow CFS\") # Plot Observed\n",
    "    lns2 = ax.plot(OutHydro['CFS'], '-', c='k', alpha=0.5, label=\"Modeled_CFS\")   # plot modeled \n",
    "    ax.set_ylabel(\"Flow CFS\")\n",
    "    ax2.set_ylabel(\"Rainfall mm\", color='b')\n",
    "    # Wierd stuff for a twinned  axis legend\n",
    "    lns = lns0+lns1+lns2; labs = [l.get_label() for l in lns]\n",
    "    ax.legend(lns, labs, loc=0)\n",
    "\n",
    "\n",
    "    # Save info\n",
    "    Param_Vals_list.append(i)\n",
    "    run_time_list.append(elapsed) \n",
    "    NSE_list.append(NSE_stat)\n",
    "    \n",
    "    \n",
    "    #### refreshing the model for each parameter run \n",
    "    # Nuke out the RUN directory to start fresh\n",
    "    for f in os.listdir(\"RUN\"):  os.remove(os.path.join(\"RUN\", f))  \n",
    "    # copy out the project filesfrom pristine to RUN  \n",
    "    copy_tree('PRISTINE_MODEL_COPY', \"RUN\") \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08759a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bf1333a",
   "metadata": {},
   "source": [
    "# Development bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set up to iterate over and run the thing \n",
    "\n",
    "# Static parameters for rain function \n",
    "Input_Precip_df = Merged_Rainfall_mm\n",
    "Precip_column_Name = \"PUU_KUKIv2\"\n",
    "ImpPrecip_units = \"mm\"\n",
    "gagFileFolder = os.path.join(\".\", \"RUN\")\n",
    "PrjName = \"Iao_v1_PrecipGag_50m\"\n",
    "Lat = \"753250.0\" \n",
    "Lon = \"2310140.0\"\n",
    "GageName = \"gag_face\"\n",
    "RainSeries_timestep_Mins = 60\n",
    "\n",
    "# Static parameters for stream obs function\n",
    "Input_Stream_df = WailukuFlow\n",
    "StreamFlow_column_Name = \"WailukuFlow\"\n",
    "StreamFlowObs_resample_Timestep_mins = 60   # note this will also be used to change HYD_FREQ in .prj file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rain gauge maker code block ready to loop over start and end dates\n",
    "\n",
    "StartDate = \"2018-02-17 00:00\"\n",
    "EndDate = \"2018-02-21 00:00\"\n",
    "\n",
    "Run_Length_mins, SlicedRainFrame = make_rain_gag_file(PrjName, Input_Precip_df, Precip_column_Name, gagFileFolder, StartDate, EndDate, Lat, Lon, \n",
    "                       GageName, RainSeries_timestep_Mins, ImpPrecip_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the .prj file to change the TOT_TIME card to whatever the total number of minutes output by the raingag function outputs\n",
    "# read in the .prj file as a 1 column csv\n",
    "df = pd.read_csv(os.path.join(\".\", \"RUN\", \"{}.prj\".format(PrjName)), names=[\"moo\"] )  # note names=moo is to make a column that will then get chopped off by numpy savetxt\n",
    "\n",
    "# ID row of and change the TOT_TIME variable\n",
    "singleCol = df.columns[0]                # this is the name of the single column\n",
    "idx_tottime = df.loc[df[singleCol].str.contains(\"TOT_TIME\", case=False)].index[0]  # this identifies the index of the TOT_TIME card\n",
    "# Set the total time value to something else. \n",
    "df.loc[idx_tottime] = \"TOT_TIME      {}\".format(Run_Length_mins)\n",
    "\n",
    "# ID row of and change the HYD_FREQ variable\n",
    "singleCol = df.columns[0]                # this is the name of the single column\n",
    "idx_HYD_FREQ = df.loc[df[singleCol].str.contains(\"HYD_FREQ\", case=False)].index[0]  # this identifies the index of the HYD_FREQ card\n",
    "# Set the total time value to something else. \n",
    "df.loc[idx_HYD_FREQ] = \"HYD_FREQ      {}\".format(StreamFlowObs_resample_Timestep_mins)\n",
    "\n",
    "# Save the df back to a prj file (the np formulation seems to write better than the pd to csv one)\n",
    "np.savetxt(os.path.join(\".\", \"RUN\", \"{}.prj\".format(PrjName)), df.values, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff92494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify roughness parameters \n",
    "\n",
    "# File where the roughness parameters are defined\n",
    "MapTableFile = os.path.join(\".\", \"RUN\", \"{}.cmt\".format(PrjName))\n",
    "\n",
    "# just using a uniform value for now\n",
    "Uniform_Val = \"0.15\"\n",
    "\n",
    "# List of parameters in the model\n",
    "Param_list = [\"-107.000000\", \n",
    "              \"-108.000000\", \n",
    "              \"-1014.000000\", \n",
    "              \"-1015.000000\", \n",
    "              \"-1016.000000\", \n",
    "              \"-1017.000000\", \n",
    "              \"-1021.000000\", \n",
    "              \"-1022.000000\"]\n",
    "\n",
    "# Switch out each parameter with the Uniform value \n",
    "for Param in Param_list:\n",
    "    cmt_prama_jama(MapTableFile, Param, Uniform_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an operational model runner code block \n",
    "\n",
    "##########    RUN THE MODEL   #################\n",
    "# Run the GSSHA process in the command line \n",
    "start_time = timeit.default_timer()     ###### Timer function start\n",
    "\n",
    "os.chdir(os.path.join(\".\", \"RUN\"))         # Chanfge into run directoy \n",
    "subprocess.call('gssha.exe {}.prj'.format(PrjName), shell=True)\n",
    "os.chdir(os.path.join(\"..\"))                # Change back out of run directory \n",
    "\n",
    "elapsed = timeit.default_timer() - start_time        ###### Timer function End, now time in seconds lives in elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing block \n",
    "\n",
    "# Process the Run's Output hydrograph into a dataframe \n",
    "OutletFile = os.path.join(\".\", \"RUN\", PrjName)+\".otl\"\n",
    "OutHydro = process_otl_file(StartDate, OutletFile)\n",
    "\n",
    "# slice the desired streamflow obs to the correct period \n",
    "SlicedStreamflow_df = Isolate_Stream_Data(Input_Stream_df, StreamFlow_column_Name, StartDate, EndDate, \n",
    "                                          StreamFlowObs_resample_Timestep_mins)\n",
    "\n",
    "# NSE calculation          # Note this has hourly resampling hard coded in, not sure if need to change someday \n",
    "#predictions = OutHydro['CFS'].to_numpy()\n",
    "#targets = SlicedStreamflow_df.resample('60T').mean().to_numpy()\n",
    "#NSE_stat = nse(predictions, np.nan_to_num(targets))\n",
    "#print(\"The NSE is {}\".format(NSE_stat))\n",
    "\n",
    "predictions = OutHydro\n",
    "targets = SlicedStreamflow_df.resample('60T').mean().interpolate()\n",
    "NSE_Frame = predictions.join(targets, how='inner')\n",
    "NSE_stat = nse(NSE_Frame['CFS'].values, NSE_Frame[StreamFlow_column_Name].values)\n",
    "print(\"The NSE is {}\".format(NSE_stat))\n",
    "\n",
    "\n",
    "# plot em \n",
    "# For some reason need to cast the rainfall as a number not object?\n",
    "SlicedRainFrame[Precip_column_Name] = SlicedRainFrame[Precip_column_Name].apply(lambda x: float(x)) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax2 = ax.twinx()\n",
    "lns0 = ax2.plot(SlicedRainFrame[Precip_column_Name], '.', c='b', alpha=0.4, label=\"Rainfall \")   # Plot rainfall \n",
    "lns1 = ax.plot(SlicedStreamflow_df[StreamFlow_column_Name], '-', c='g', alpha=0.5, label=\"Observed Waihehe flow CFS\") # Plot Observed\n",
    "lns2 = ax.plot(OutHydro['CFS'], '-', c='k', alpha=0.5, label=\"Modeled_CFS\")   # plot modeled \n",
    "ax.set_ylabel(\"Flow CFS\")\n",
    "ax2.set_ylabel(\"Rainfall mm\", color='b')\n",
    "# Wierd stuff for a twinned  axis legend\n",
    "lns = lns0+lns1+lns2; labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812833e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822a169",
   "metadata": {},
   "source": [
    "rando notes\n",
    "Note at 60 sec time step = 208 sec \n",
    "at 30 sec step = 219 sec\n",
    "at 15 = 204 sec \n",
    "at 5 sec = 244 sec \n",
    "at 15 sec but Quiet card = 207 sec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d6dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
